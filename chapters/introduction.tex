\chapter{Introduction}\label{chap:introduction}

\section{Motivation}\label{sec:motivation}

The amount of information produced and processed in the world has seen a steady increase over the past decades.
The world's internet traffic has been increasing exponentially.
Edholm's Law \cite{Edholm04} predicts that this behavior should continue until at least 2030.
This law is an observation that the total data transmitted globally rises in a Moore's Law-like pattern \cite{Moore98}: it doubles every $18$ months.

As the amount of data generated by users all around the world increases, the need to store and access this data increases accordingly.
Alongside this evolution there is an expansion on the number of digital services offered to users: musics, videos, books and other types of files and media that can be accessed from anywhere.
So, not only data volume increases, but it has to be accessible more reliably than ever.

The solution companies found to provide these services globally is called cloud Computing, which can be defined as ``a technique where IT services are provided by massive low-cost computing units connected by IP networks'' \cite{Qian09}.

However, even though cloud Computing is sold as a way for users to access computing resources they do not have physical access to, the hardware responsible for this computing power must exist somewhere.
This is achieved by the establishment of multiple data centers all around the world to which devices from anywhere can connect in order to get access to the desired services.
Microsoft, for example, has 300 data centers worldwide to provide their services to their clients \cite{MicrosoftDataCenters}.

A data center is a complex installation that consists of thousands of hard drives connected by kilometers of optical fiber cables.
Therefore, there are massive amounts of investment done in order to create and maintain these facilities.
Microsoft alone is investing \$80 billion in order to upgrade their data centers \cite{MicrosoftDataCenters}.
So there is a demand for services related to the maintenance and improvement of these data centers.

In this context, one of the main aspects of cloud computing in general is the strong virtualization and reliability of the system \cite{Qian09}, meaning that even if some components break the service should still be provided without interruption.
So, as hardware components fail, they need to be replaced to ensure that the system keeps working.

Moreover, to maintain a reliable system it is not enough to only replace the hardware after it fails.
In order to prevent some serious issues, such as data loss, without having to permanently have a copy of all the data it is necessary to predict the hardware failures.

Therefore, one of the main problems faced by data centers is the need to detect which pieces of hardware are going to fail before a fatal crash occurs.
The identification of a soon to fail component makes it possible for it to be replaced beforehand.
This allows both the prevention of data loss and the maintenance of the service reliability.

Out of all the components that are part of a computer, $80\%$ of the failures occur due to the problems on the hard drives, as indicated by Table \ref{devicefailuretable}.
Therefore, there is a special focus on predicting failures on disks.

\begin{table}
    \begin{center}
      \includegraphics[width=.6\linewidth]{FailureProportions.png}
      \caption[Failure percentage by component]{Data center failure percentage by component - \cite{Wang17}}
      \label{devicefailuretable}
    \end{center}
  \end{table}

Hardware vendors are well aware of this situation and the difficulties related to managing thousands of hard drives.
So, in order to allow their users to better tackle these problems, they added a monitoring system to their drives.
This technology is called SMART (Self-Monitoring, Analysis, and Reporting Technology).

Using this protocol, vendors can provide to users indicators of the disk status.
These attributes include Power-On Hours, Air Flow Temperature and Reallocated Sector Count (number of sectors that had to be copied elsewhere on the disk after a failed read or write operation in order to prevent data loss) \cite{SamsungSSD}.

These disks may also come equipped with a software that is, in theory, able to signal problems with the disk based on the values of the SMART attributes.
However, these systems just compare each SMART value to a threshold that is set by the vendor for each attribute \cite{SamsungSSD}.

Moreover, even when a threshold has been exceeded and a disk is flagged by the software, it does not mean that the disk will suffer an unrecoverable failure.
It is often just a signal that the hard drive will work more slowly than its specifications indicate.

In addition to that, this status provided by the algorithm developed by the vendors consider each attribute independently.
They do not take into account the fact that some problems in the hard drive can cause different indicators to increase simultaneously.

As a consequence, considering the relationship between different attributes would allow problems to be more accurately detected.
For example, having a huge number of Power-On Cycles when compared to Power-On Hours may indicate a problem in the computer that causes it to crash and restart constantly.

Finally, this \textit{ad-hoc} approach uses static values and do not consider the evolution of the attributes over time.
Imagine the situation of a disk that has not reallocated any sectors so far, and at some point the rate at which sectors have to be reallocated increases abruptly.
By taking into account how this attribute evolves over time, this problem can be more quickly detected before the threshold value is reached.
In this case it would suffice to add a threshold value to the change rate value too.

However, even though the built-in failure detection system is not very effective, it does not mean that the data itself cannot be used to obtain more interesting results.
Current research uses mostly machine learning approaches such as support vector machines and neural networks to tackle this problem.
Further details of how these methods work and how they can be applied to the problem at hand will be detailed in Chapter \ref{chap:background}. 

\section{Problem}\label{sec:problem}

In this section we formally define the parameters of the problem, the data available and the desired output that an algorithm or model that tackles the hard drive failure prediction problem should produce.

Suppose we have a dataset consisting of SMART attribute samples for a certain time period of the disks in a data center.
During the observation interval, we also record if each one of the disks has failed in an unrecoverable manner or not.

The challenge is to train a model able to learn patterns related to the failing disks well enough to be able to identify them before the failure occurs while not flagging the working ones.

Let $x_{i,t}$ be the vector and its $j^{th}$ component designs the measure of the $j^{th}$ SMART attribute for disk $i$ at time $t$.
For the disks we have observed, we also know a value $y_i$ that is either $0$, if this disk has eventually failed or $1$ if, until the end of the observation period, the disk kept working properly.
Also, let the number of different smart attributes be equal to $m$.

Then given a series of vectors with the SMART attributes for an unseen disk, $\left(\hat{x}_1\dots\hat{x}_t\right)$, we want to output a value $\hat{y}_t$.
Here, $\hat{y}_t$ equal to 0 means that the model predicts that the disk that generated the given sequence is going to fail soon.
A value of $\hat{y}_t$ equal to 1 denotes that there isn't a risk of the hard drive failing in the near future.

In a real world scenario, if the value $\hat{y}_t$ is equal to $0$, then a warning should be sent to the people responsible for maintaining the data center in order to replace the concerned disk.

This corresponds to a particular kind classification problem.
Instead of classifying individual samples $\hat{x}_i$, we intend to classify a sequence of them $\left(\hat{x}_1\dots\hat{x}_t\right)$ as representing a working or failing disk.

Therefore, machine learning methods such as decision trees and neural networks, provide a start point to begin tackling the problem.
However, some additional tools need to be developed in order to obtain a verdict for the sequence from the samples that form it.

Here, it is important to describe some particularities of the problem that will allow us to better tackle it.
First, as we have mentioned, the samples form a time series.
As we will see on Chapter \ref{chap:background}, this can be a motivation to use methods such as Long Short-Term Memory networks (LSTM) that can encode and work with time dependencies.

Moreover, even other methods such as Random Forests that cannot easily deal with time dependencies can be extended to make use of the time series aspect of the data.
This is due to the fact that we are able to encode some aspects of the time series and append it to each sample.
More specifically, as we will see in Subsection \ref{subsec:change_rate}, we are able to add information regarding the change rate of the attributes to each sample.

A second point that needs to be mentioned is that the amount of disks in each class is not similar at all.
Since a hard drive has a service life of around 3 to 5 years \cite{Vishwanath10}, and the observations used to build datasets are usually performed over the interval of a few weeks, no failure will be observed for most of the disks.
In a typical dataset, the ratio of failing disks over the total amount is between $0.4\%$ and $1.9\%$ \cite{Xu16}.
So, any approach to this problem has to handle this imbalance in the input data.

Thirdly, the meaning of each SMART attribute is not the same for different vendors \cite{SamsungSSD}.
The protocol is standardized, but what it reports is not.
So, an algorithm for failure prediction cannot be trained on data from different vendor disks.
Moreover, in general, it is not a good idea to mix data from different disks on the same dataset, since they can have different failure causes profiles.

Most of the time, the limitation of not being able to mix data about different disk models does not pose a problem to the datasets generated by the data centers.
This is due to the fact that, in practice, a data center uses hundreds of copies of the hardware of the same model and has at most a couple of different models.
This allows for the replacement of failing disks do be done in a cheaper and more swiftly manner, since it simplifies stock management.
Consequently, usually there is enough data for any model of disks used in a real world scenario.

The performance of the different approaches to this problem is done using two main metrics.
The first one is the Failure Detection Rate (FDR).
This is equal to the ratio between the number of disks that are correctly predicted to be failing and the total amount of failing disks.
Its value should be as close to $1$ as possible.

The second metric is the False Alarm Rate (FAR).
This is equal to the ratio between the number of disks that are wrongly predicted to be failing and the total amount of disks that do not fail.
Its value should be as close to $0$ as possible.

The challenge is to find algorithms that find an equilibrium between these values.
If it predicts every $\hat{x}_t$ to give an output $\hat{y} = 1$, the FAR will be equal to $0$, but the FDR too.
On the other hand, if it predicts every $\hat{x}_t$ to give an output $\hat{y} = 0$, the FDR will be equal to $1$, but the FDR too.
So, we notice that a tradeoff must be done between the FDR and the FAR.

Finding this balance is identical to the Precision-Recall tradeoff \cite{buckland1994relationship} in most machine learning problems.
This is because, as we are going to show in Section \ref{sec:lstm}, the FDR and the FAR can be mapped to the precision and the recall metrics respectively.

However, both metrics should be treated identically.
Suppose that during a period of one month, $2\%$ of the hard drives in a data center fail.
This corresponds to a lifetime of about 4 years, so it can represent a real world scenario \cite{Vishwanath10}.
If the FDR decreases by $1\%$, then only an additional $0.02\%$ of the total of the disks of the data center will fail without a warning during one month.

In contrast, if the FAR increases by the same amount, then $0.98\%$ of the disks of the whole data center will be unnecessarily replaced during the same period.
This is close to $50\%$ of the disks that actually need to be replaced and can incur a substantial cost.
So, most algorithms in the literature prefer to have a bias towards classifying a disk as working rather than failing.

Additionally, it is impossible to reach perfect values for both indicators.
This is due to the fact that hard drives are subject to real world conditions.
For example, a short circuit on a disk may be caused by an electrical surge, but it does not depend on the internal state of the disk and thus cannot be predicted by any approach using SMART attributes.

In order to formally define the FAR and the FDR, let the positive event be the one of the hard drive failing and negative the one of it not failing.

Then the true positives are the hard drives whose failure was correctly predicted.
The true negative are the hard drives that did not fail and that were not flagged as going to fail.
The false negatives are the hard drives that failed but were not flagged as so.
And, the false positives are the hard drives that did not fail, but that were predicted as going to fail.
The cardinalities of these four sets are denoted $TP$, $TN$, $FN$ and $FP$ respectively.

This allows us to rewrite the FDR and the FAR as follows:

\begin{equation}\label{eq:fdr_far}
    \begin{cases}
        \begin{aligned}
            FDR &= \dfrac{TP}{TP + FN} \\
            FAR &= \dfrac{FP}{FP + TN}
        \end{aligned}
    \end{cases}
\end{equation}

By checking the values of some popular metrics used in machine learning \cite{olson2008advanced}, we can see that the FDR and the FAR can be written in terms of other metrics.
In fact, the FDR is equivalent to the recall and the FAR equates to $1-TNR$, where the $TNR$ is the true negative rate and is given by $\dfrac{TN}{TN + FP}$.
This relationship can be deduced as follows:

\begin{equation}
  FAR = \dfrac{FP}{FP + TN} = \dfrac{(FP + TN) - TN}{FP + TN} = 1 - \dfrac{TN}{FP + TN} = 1 - TNR 
\end{equation}

There is a third metric we can use to evaluate our models, even though it is usually secondary when compared to the others.
The Time In Advance (TIA) is how much time there was between the moment in which a model warned that a disk was going to fail and the instant it actually did.

The TIA measures how much time, on average, the people responsible for managing a data center have to back up the data from a given disk and replace it to prevent any form of data loss or disruption of service.

The TIA is less important than the FDR and FAR because we do not need to try to maximize it.
It suffices for it to be high enough to allow the team to respond.

In order to prevent the problems mentioned above, oftentimes actions have to be taken considering the worst case scenario.
For the TIA to be large for almost every scenario it must have a large average while keeping a small standard deviation.

The average of the TIA can be formally describing by considering all samples belonging to the true positives.
We then record the moment in which the disk $i$ precisely failed $T_i$.
Then, let $pred((x_{i,1}, \dots ,x_{i,t}))$ be the prediction of the model being evaluated for the $t$ first samples of the disk $i$.
Then, the average of the TIA is given by the following formula:

\begin{equation}
  \overline{TIA} = \dfrac{\sum_{i\in \textbf{ true positive}} \max\{T_i - t \mid pred((x_{i,1},\dots,x_{i,t})) = 1, t \geq 1\}}{TP}
\end{equation}

\section{Objectives}

This project has three main objectives.
The first one is to create a library that implement some state of the art methods used in research.
The idea is also to develop it in such a way that it can be extended with new methods and models in order to allow them to be easily tested and compared to other ones. 

After creating the library, further tests can be performed on existing methods to understand how they evolve as their hyperparameters change.
For example, most works, such as \cite{Zhu13} and \cite{Xu16}, use different pre- and post-processing steps and therefore make it harder for them to be objectively compared.
Our intention is to remove any bias that can occur due to these stages.

The third objective is to extend existing methods to also make use of the concept of health status.
The idea is to use the fact that, even though the sequences must be classified in two classes, the samples that form them do not.

So, we increase the number of classes by putting training samples closer to the moment of failure in a different class than the ones that are farther away.
This step will be explained in details on Section \ref{sec:health_status}.

So, the goal is to extend our methods to support more than two classes and study their reaction to this change.